# -*- coding: utf-8 -*-
"""DeploymentAPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W43gY-yIUbfkj97oI2lUSp5M17sar94M

#**Despliegue del Modelo: Machine Learning - Statistical Approach/Feature Extraction**
---
"""

!pip install fastapi

#Install librarys
from IPython.display import display
from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import pandas as pd
import re
import os
import h5py
import scipy.io
import matplotlib.pyplot as plt
import urllib.request
# Upgrade scikit-learn the final version
!pip install -U scikit-learn
# Import scikit-learn
import sklearn
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import matplotlib as mpl
import seaborn as sns
# Librería de visualización interactiva - Plotly
!pip install -U plotly
import plotly
import plotly.express as px
import plotly.graph_objects as go

# Commented out IPython magic to ensure Python compatibility.
# Import the warnings module to manage warning messages
import warnings
# Set the filter action to ignore warnings
warnings.simplefilter(action='ignore')
# Enable inline plotting in Jupyter Notebook or JupyterLab
# %matplotlib inline
# Set the figure format to 'retina' for high-resolution figures
# %config InlineBackend.figure_format = 'retina'
# Set the DPI (dots per inch) of the figures
mpl.rcParams['figure.dpi'] = 105
# Set the default size of the figures
mpl.rcParams['figure.figsize'] = (9, 7)
# Set the default theme for Seaborn
sns.set_theme()

"""## **GPR-Raw Data to Tensor Data**
----
"""

# Read the .h5 file-pandas DataFrame

# Load data from the first HDF5 file
with h5py.File('/content/drive/Shareddrives/TII UNAL GPR/Machine Learning Models/Data Training/Raw Data/datas.h5', 'r') as f:
    X = f['datas'][:]

# Load data from the second HDF5 file
with h5py.File('/content/drive/Shareddrives/TII UNAL GPR/Machine Learning Models/Data Training/Raw Data/labels.h5', 'r') as f:
    y = f['labels'][:]

# Time domain feature extraction methods - statistical approach
import scipy.stats as stats
from scipy.stats import skew
from scipy.stats import kurtosis

Lim=[10, 90]
mean = np.zeros((len(X), Lim[1]-Lim[0]))
std = np.zeros((len(X), Lim[1]-Lim[0]))
var = np.zeros((len(X), Lim[1]-Lim[0]))
rms = np.zeros((len(X), Lim[1]-Lim[0]))
abs_max = np.zeros((len(X), Lim[1]-Lim[0]))
skewness = np.zeros((len(X), Lim[1]-Lim[0]))

con = np.zeros((len(X), (Lim[1]-Lim[0])*6))

for x in range(len(X)):
# add the vectors by column
  rw_mean = np.mean(X[x].T, axis=0)
  rw_std = np.std(X[x].T, axis=0)
  rw_var = np.var(X[x].T, axis=0)
  rw_rms = np.sqrt(np.mean(np.square(X[x].T), axis=0))
  rw_abs_max = np.max(np.abs(X[x].T), axis=0)
  rw_skewness = skew(X[x].T, axis=0)


  mean[x]=rw_mean[Lim[0]:Lim[1]]
  std[x]=rw_std[Lim[0]:Lim[1]]
  var[x]=rw_var[Lim[0]:Lim[1]]
  rms[x]=rw_rms[Lim[0]:Lim[1]]
  abs_max[x]=rw_abs_max[Lim[0]:Lim[1]]
  skewness[x]=rw_skewness[Lim[0]:Lim[1]]

  con[x]=np.concatenate([mean[x], std[x], var[x],rms[x],abs_max[x],skewness[x]])

Xdata=con
print(f' {Xdata.shape}')
print(f'{y.shape}')
abs_max.shape

"""## **Training Data - Full Data**
----

```
# This is formatted as code
```


"""

# Data train 70%
X_train, X_test, y_train, y_test = train_test_split(Xdata, y, test_size=0.3, random_state=2)
X_test.shape

"""## **Model: Random Forest**
----

"""

from sklearn.ensemble import RandomForestClassifier
# Create an instance of RandomForestClassifier with balanced class weights
rf_classifier = RandomForestClassifier(n_estimators=150, class_weight='balanced')

# Fit the classifier to your training data
rf_classifier.fit(X_train, y_train)

# Make predictions on test data
y_pred = rf_classifier.predict(X_test)

# Evaluate the performance of the classifier
accuracy = rf_classifier.score(X_test, y_test)
print("Accuracy Random Forest:", accuracy)

"""Se utiliza la herramienta joblib para guardar el modelo:"""

import joblib
joblib.dump(rf_classifier,"rf_classifier.joblib")

"""## **FastAPI**
----

Definición de la entrada API como una clase pydantic
"""

from pydantic import BaseModel
from typing import List

class ApiInput(BaseModel):
    text: List[str]

"""Se define una salida para la salida del API"""

class ApiOutput(BaseModel):
    is_hate: List[int]

"""Definicion del EndPoint con FastAPI."""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from fastapi import FastAPI # importamos el API
# from pydantic import BaseModel
# from typing import List
# import joblib # importamos la librería para cargar el modelo
# 
# class ApiInput(BaseModel):
#     texts: List[str]
# 
# class ApiOutput(BaseModel):
#     is_hate: List[int]
# 
# app = FastAPI() # creamos el api
# rf_classifier = joblib.load("rf_classifier.joblib") # cargamos el modelo.
# 
# @app.post("/IED") # creamos api que permita requests de tipo post.
# async def create_user(data: ApiInput) -> ApiOutput:
#     predictions = rf_classifier.predict(X_test) # generamos la predicción
#     preds = ApiOutput(is_IED=predictions) # estructuramos la salida del API.
#     return preds # retornamos los resultados

"""Despliegue del API"""

# Commented out IPython magic to ensure Python compatibility.
!mkdir mlapi
!mv main.py rf_classifier.joblib mlapi/
# %cd mlapi/

"""Inicio del repositorio"""

!git config --global user.email "jarangelr@unal.edu.co"
!git config --global user.name "AlejandroRR"
!git config --global init.defaultBranch master
!git init

"""Requerimientos"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# scikit-learn
# fastapi
# uvicorn

"""Archivo de configuracion Railway"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile railway.json
# {
#   "$schema": "https://railway.app/railway.schema.json",
#   "build": {
#     "builder": "NIXPACKS"
#   },
#   "deploy": {
#     "startCommand": "uvicorn main:app --host 0.0.0.0 --port $PORT",
#     "restartPolicyType": "ON_FAILURE",
#     "restartPolicyMaxRetries": 10
#   }
# }

"""Se agragan los archivos"""

!git add railway.json requirements.txt main.py rf_classifier.joblib
!git commit -m "Agregamos los archivos del API."

token = "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"  # Agregue su token dentro de las comillas.
repo_url = "https://https://github.com/Fabianunal/Proyecto_msdl6.git" # Agregue la url de su repositorio dentro de las comillas.

import re
pat = re.compile(r"(https://)(.*)")

import os
match = re.match(pat, repo_url)
url_token = "".join([match.group(1), token, "@", match.group(2)])
os.environ["GITHUB"] = url_token

!git remote add origin $GITHUB

!git push origin master

model_url = "https://mlapi-production-d908.up.railway.app" # Agregue acá la url de railway